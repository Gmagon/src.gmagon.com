---
title: How AI is learning to see?
---

If you've noticed a huge surge in companies saying they're using artificial intelligence lately, you aren't alone. Even simple-sounding tech is suddenly "built on AI", such as the hotdog identifier [Not Hotdog](https://www.engadget.com/2017/06/26/silicon-valley-not-hotdog-app-android/?utm_campaign=The%20Visionary&utm_source=Engadget&utm_medium=sponsored-story) or Google's Allo chat app, which turns selfies into [custom emoji](https://www.engadget.com/2017/05/11/google-allo-machine-learning-emoji-steven-universe/?utm_campaign=The%20Visionary&utm_source=Engadget&utm_medium=sponsored-story).

If you're the skeptical type, you're probably distrustful of these claims. Buzzwords like "big data" and "cloud processing" were used by pretty much every startup in their heyday, too. But some of these whimsical apps are surprisingly legit.

Not Hotdog, for instance, is [built on](https://www.engadget.com/sponsoredcontent/23023886-hotdogs-emoji-and-faces-how-ai-is-learning-to-see/?utm_campaign=The%20Visionary&utm_source=Engadget&utm_medium=sponsored-story) top of Google's machine learning library, Tensorflow, plus the open source Keras neural network. Over several months, its developers tested multiple frameworks, trained their AI on edge cases, and figured out how to apply the results using only mobile phone processors.

Small applications like Not Hotdog offer a window into the larger challenge of accurately tagging thousands of distinct objects, which we've spent years on at [GumGum](http://www.gumgum.com/?utm_campaign=The%20Visionary&utm_source=Engadget&utm_medium=sponsored-story). Eye color, for example, is tricky for machines. A convolutional neural network learns to recognize eyes by breaking images of faces into pieces, examining each piece separately, then merging the resulting data to consider as a whole. Do the pieces have characteristics of a face? If so, which is the eye, and what color is it? Once it has the answer, we can overlay a glow of the correct color and size on a person's iris in an image.  
![](https://s.aolcdn.com/hss/storage/midas/1dbeb9f53a89939d502d37917f257ac8/205461185/Picture1.png)While each neural network is its own unique model, some details are consistent: neural networks, for instance, loosely mimic the way our visual cortex processes visual information, thus some of the underlying mathematics is shared across applications. Several of the most important ideas can be learned from [The Visionary's](http://www.thevisionary.com/?utm_campaign=The%20Visionary&utm_source=Engadget&utm_medium=sponsored-story) micro-course on machine learning and computer vision.

So the profusion of computer vision apps and developers working on them actually couldn't come at a better time. For more of the developments in computer vision, we're building a resource at [The Visionary](http://www.thevisionary.com/?utm_campaign=The%20Visionary&utm_source=Engadget&utm_medium=sponsored-story)â€”come give us a look, or subscribe to our newsletter, for more.

Source:[https://www.engadget.com/sponsoredcontent/23023886-hotdogs-emoji-and-faces-how-ai-is-learning-to-see/](https://www.engadget.com/sponsoredcontent/23023886-hotdogs-emoji-and-faces-how-ai-is-learning-to-see/)

